# aws s3 접근하기

`test.py` 를 생성합니다.

```sh
vi test.py
```

```python
# -*- coding: utf-8 -*-
import pyspark

sc = pyspark.SparkContext.getOrCreate()
sc.setSystemProperty("com.amazonaws.services.s3.enableV4", "true")

hadoopConf = sc._jsc.hadoopConfiguration()
hadoopConf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
hadoopConf.set("fs.s3a.endpoint", 's3.ap-northeast-2.amazonaws.com')

# 확장자는 csv, gz 등이 가능하다.
df = sc.textFile("s3a://skyer9-test/sample_us.tsv").map(lambda x: x.split("\t"))

print('\n\n------------------------------------------------\n\n')
print(df.take(10))
print('\n\n------------------------------------------------\n\n')

sc.stop()
```

## jar 파일 다운받아 설치하기

특별한 이유가 없으면 최신버전 말고 아래 버전을 다운받습니다.

``` sh
wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.6/hadoop-aws-2.7.6.jar
flintrock copy-file bigdata-cluster aws-java-sdk-1.7.4.jar ./spark/jars/
flintrock copy-file bigdata-cluster hadoop-aws-2.7.6.jar ./spark/jars/
```

## 테스트 파일 실행하기

[sample_us.tsv](https://s3.amazonaws.com/amazon-reviews-pds/tsv/sample_us.tsv) 에서 파일을 다운받아 s3 에 업로드합니다.

```sh
flintrock copy-file bigdata-cluster test.py ./

# 마스터 노드에 로그인
flintrock login spark
```

```sh
spark-submit --master spark://172.31.30.40:7077 \
               --conf spark.hadoop.fs.s3a.endpoint=s3.ap-northeast-2.amazonaws.com \
               --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
               --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4=true \
               --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4=true \
               --conf spark.hadoop.fs.s3a.access.key=XXXXXXXXXXXXXXXXXXXXXX \
               --conf spark.hadoop.fs.s3a.secret.key=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX \
               test.py
```
